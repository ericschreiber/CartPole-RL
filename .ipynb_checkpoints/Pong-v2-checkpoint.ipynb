{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7043f946-667a-46c0-9736-073b210f5c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy #Probalistic approch to also try some new paths from time to time\n",
    "from rl.memory import SequentialMemory\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2618a7ae-997a-42a8-afa9-25fc434b50d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f068d80b-f708-47a2-8f91-ebef74a343fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gym wrapper\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "        \n",
    "    def observation(self, obs):\n",
    "        return ProcessFrame84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 210 * 160 * 3:\n",
    "            img = np.reshape(frame, [210, 160,  3]).astype(np.float32)\n",
    "        elif frame.size == 250 * 160 * 3:\n",
    "            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\"  \n",
    "\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 +  img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15b0e81-2b26-4eaa-9a20-71c2ab5ef386",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FireResetEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(FireResetEnv, self).__init__(env)\n",
    "        assert env.unwrapped.get_action_meanings()[1] == \"FIRE\"\n",
    "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
    "        \n",
    "    def step(self, action):\n",
    "        return self.env.step(action)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(1)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        obs, _, done, _ = self.env.step(2)\n",
    "        if done:\n",
    "            self.env.reset()\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ee5422f-4730-4278-bc68-f136eef25314",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "        \n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a3b7896-03fa-445b-a970-b448f3c79173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low,\n",
    "        dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        #self.buffer[:-1] = self.buffer[1:]\n",
    "        #self.buffer[-1] = observation\n",
    "        #return self.buffer\n",
    "        return observation\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8e9914f-8bfb-4b6e-b49d-b59e1aaea327",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "     def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a66120c-52a2-4cb1-8c67-3f7849c41a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erics\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\spaces\\box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pong-v0')#endlich l√§uft es --endlich--\n",
    "\n",
    "env = MaxAndSkipEnv(env)\n",
    "env = FireResetEnv(env)\n",
    "env = ProcessFrame84(env) \n",
    "env = BufferWrapper(env, 4)\n",
    "env = ScaledFloatFrame(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f26eb2c9-e269-4366-8762-46c14993fc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b048ac38-24dd-4041-bffe-7b0a3e2cd80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States:336, Actions:6\n"
     ]
    }
   ],
   "source": [
    "print('States:{}, Actions:{}'.format(states, actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbcd0e0b-d461-4889-9582-ccf39493c9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x152070416d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "print(state.ndim)\n",
    "plt.imshow(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "161b4845-6835-4c5e-9632-4c7bd27d8a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random inputs for visualisation\n",
    "for episodes in range(0):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([2,3])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episodes:{} Score:{}'.format(episodes, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f94fd285-58dc-4be2-a02e-edaf18df9640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buildt model: input layer -> 2x fully connected -> output layer\n",
    "#The input to the neural\n",
    "#network consists is an 84 √ó84 √ó4 image produced by œÜ. The first hidden layer convolves 16 8 √ó8\n",
    "#filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18]. The second\n",
    "#hidden layer convolves 32 4 √ó4 filters with stride 2, again followed by a rectifier nonlinearity. The\n",
    "#final hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fully-\n",
    "#connected linear layer with a single output for each valid action. The number of valid actions varied\n",
    "#between 4 and 18 on the games we considered. We refer to convolutional networks trained with our\n",
    "#approach as Deep Q-Networks (DQN). [Playing Atari with Deep Reinforcement Learning] \n",
    "#I will do it with one image bc its quite late and I want to leave something to do for another day:)\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8),input_shape=(1,84,84,1), strides = 4, activation = 'relu'))\n",
    "    model.add(Conv2D(64, (4, 4), strides = 2, activation = 'relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides = 1, activation = 'relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    #model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48dfd101-005f-4056-a7f1-723d88af2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    #dqn = DQNAgent(model=model, policy=policy, nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6a9302e-fde9-484a-bb2d-feb53a1d7a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 1, 20, 20, 32)     2080      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 1, 9, 9, 64)       32832     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 1, 7, 7, 64)       36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,681,062\n",
      "Trainable params: 1,681,062\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c1eca6b-8e1b-4be3-a8b1-b71632fb8f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8026fa7f-b162-4472-af2b-2b52b713fa02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "    8/10000 [..............................] - ETA: 2:45 - reward: 0.0000e+00"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erics\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   50/10000 [..............................] - ETA: 15:08 - reward: -0.0600done, took 4.801 seconds\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'): #please use my GPU if it would recognise it :( (Probably is a problem with jupyter notebook)\n",
    "    dqn.fit(env, nb_steps=5000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b168e85-1c96-442a-9fc5-a6f62d2f6dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in ~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\rl\\agents\\dqn.py \n",
    "# changed q_values = self.model.predict_on_batch(batch) (in line 63)\n",
    "# to              batch = batch[0, :, :, :, :] (added line)\n",
    "#\n",
    "# additionally line 297 changed to target_q_values = self.target_model.predict_on_batch(state1_batch[0,:,:,:,:])\n",
    "# from target_q_values = self.target_model.predict_on_batch(state1_batch)\n",
    "\n",
    "# changed all back to original\n",
    "# thinking that env adds [:,x,:,:,:] the x dimension. But i dont know why... ([sample number, ??? , dim picture, dim picture, rgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dff872ce-17f1-4d5b-9f68-4e92a3c2dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = r'C:\\Users\\erics\\Documents\\Programme\\RL Beispiel Projekt\\Model_Checkpoints'\n",
    "model.save_weights(checkpoint_path.format(epoch=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdfc6361-17a9-4542-aa83-c15a4bf7d46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: -21.000, steps: 248\n",
      "Episode 2: reward: -21.000, steps: 254\n",
      "Episode 3: reward: -21.000, steps: 255\n",
      "Episode 4: reward: -21.000, steps: 253\n",
      "Episode 5: reward: -21.000, steps: 251\n",
      "Episode 6: reward: -21.000, steps: 249\n",
      "Episode 7: reward: -21.000, steps: 253\n",
      "Episode 8: reward: -21.000, steps: 255\n",
      "Episode 9: reward: -21.000, steps: 254\n",
      "Episode 10: reward: -21.000, steps: 254\n",
      "-21.0\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=10, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb1c60e1-e39c-4a82-a5ac-2bf8b897585f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x152070b06d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = r'C:\\Users\\erics\\Documents\\Programme\\RL Beispiel Projekt\\Model_Checkpoints'\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330b51c-16a0-4153-b125-093bf2f3d067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
