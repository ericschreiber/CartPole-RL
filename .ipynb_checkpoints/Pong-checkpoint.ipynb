{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d531e515-7511-41ba-950c-f27d719ff0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy #Probalistic approch to also try some new paths from time to time\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "# import necessary modules from keras\n",
    "import keras\n",
    "from keras.models import InputLayer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26bc6d45-f1db-4b73-a8fd-74c9e8fbe939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ale_py import ALEInterface, SDL_SUPPORT\n",
    "import atari_py\n",
    "\n",
    "ale = ALEInterface()\n",
    "ale.setInt(\"random_seed\", 123)\n",
    "\n",
    "ale.loadROM(atari_py.get_game_path('breakout'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d646366e-14b4-4142-ad20-345987ee0c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SDL_SUPPORT:\n",
    "    ale.setBool(\"sound\", True)\n",
    "    ale.setBool(\"display_screen\", True)\n",
    "\n",
    "# Load the ROM file\n",
    "#rom_file = sys.argv[0]\n",
    "#ale.loadROM(rom_file)\n",
    "\n",
    "# Get the list of legal actions\n",
    "legal_actions = ale.getLegalActionSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a678cda-bce9-40b6-a0b6-b009f3204aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Action.NOOP: 0>, <Action.FIRE: 1>, <Action.UP: 2>, <Action.RIGHT: 3>, <Action.LEFT: 4>, <Action.DOWN: 5>, <Action.UPRIGHT: 6>, <Action.UPLEFT: 7>, <Action.DOWNRIGHT: 8>, <Action.DOWNLEFT: 9>, <Action.UPFIRE: 10>, <Action.RIGHTFIRE: 11>, <Action.LEFTFIRE: 12>, <Action.DOWNFIRE: 13>, <Action.UPRIGHTFIRE: 14>, <Action.UPLEFTFIRE: 15>, <Action.DOWNRIGHTFIRE: 16>, <Action.DOWNLEFTFIRE: 17>]\n"
     ]
    }
   ],
   "source": [
    "print(legal_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6b424e4-4d5e-4ea6-9f19-34a97be42563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 ended with score: 2\n",
      "Episode 1 ended with score: 1\n",
      "Episode 2 ended with score: 0\n",
      "Episode 3 ended with score: 2\n",
      "Episode 4 ended with score: 0\n",
      "Episode 5 ended with score: 1\n",
      "Episode 6 ended with score: 4\n",
      "Episode 7 ended with score: 2\n",
      "Episode 8 ended with score: 2\n",
      "Episode 9 ended with score: 0\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10):\n",
    "    total_reward = 0\n",
    "    while not ale.game_over():\n",
    "        a = legal_actions[random.randrange(len(legal_actions))]\n",
    "        # Apply an action and get the resulting reward\n",
    "        reward = ale.act(a)\n",
    "        total_reward += reward\n",
    "    print(\"Episode %d ended with score: %d\" % (episode, total_reward))\n",
    "    ale.reset_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f7a466-b6c2-4542-9e51-8722c446bec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')#endlich läuft es --endlich--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d45f2f-1231-495c-a87f-463a0c6e39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e925f68-b4e5-44de-a628-9a6ac1fb4dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States:210, Actions:6\n"
     ]
    }
   ],
   "source": [
    "print('States:{}, Actions:{}'.format(states, actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61aac356-b908-4dd5-aee5-cba5daaa59e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\erics\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\envs\\atari\\environment.py:255: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\erics\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Der Threadmodus kann nicht nach dem Einstellen geändert werden\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes:0 Score:-21.0\n",
      "Episodes:1 Score:-21.0\n",
      "Episodes:2 Score:-20.0\n",
      "Episodes:3 Score:-21.0\n",
      "Episodes:4 Score:-21.0\n",
      "Episodes:5 Score:-20.0\n",
      "Episodes:6 Score:-19.0\n",
      "Episodes:7 Score:-19.0\n",
      "Episodes:8 Score:-20.0\n",
      "Episodes:9 Score:-21.0\n"
     ]
    }
   ],
   "source": [
    "#random inputs for visualisation\n",
    "for episodes in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = random.choice([2,3])\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "    print('Episodes:{} Score:{}'.format(episodes, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13b3c4f5-c133-42d2-8a4d-3223aeee8f28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18f65f3fb80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "state = env.reset()\n",
    "plt.imshow(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b65045f-eaf2-4db0-a85c-66c63d162291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x18f5d381220>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepro(I):\n",
    "    # prepro 210x160x3 frame into 6400 (80x80) 1D float vector \n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(float).ravel()#Show preprocessedobs_preprocessed = prepro(observation).reshape(80,80)\n",
    "\n",
    "plt.imshow(n_state, cmap='gray')\n",
    "\n",
    "prepro_state = prepro(n_state).reshape(80,80)\n",
    "plt.imshow(prepro_state, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1fd922-8d04-4329-a20e-4bc4da8be1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma):\n",
    "    # take 1D float array of rewards and compute discounted reward \n",
    "    r = np.array(r)\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "\n",
    "    for t in reversed(range(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset \n",
    "    running_add = running_add * gamma + r[t] \n",
    "    discounted_r[t] = running_add\n",
    "    discounted_r -= np.mean(discounted_r) #normalizing the result\n",
    "    discounted_r /= np.std(discounted_r) #idem using standar deviation\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52b11d-556c-4f3b-98d5-76d59a921fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()# hidden layer takes a pre-processed frame as input, and has 200 units. Simple layer architectur of 200 x1, 1x1\n",
    "model.add(Dense(units=200,input_dim=80*80, activation='relu', kernel_initializer='glorot_uniform'))# output layer — we use a Sigmoid here, in order to get a 0, or 1 value to represent ACTION UP\n",
    "model.add(Dense(units=1, activation='sigmoid', kernel_initializer='RandomNormal'))# compile the model using traditional Machine Learning losses and optimizers\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083187d8-812e-41a6-b954-6c45ffd7894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "observation = env.reset()\n",
    "prev_input = None\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3\n",
    "\n",
    "gamma = 0.99# initialization of variables used in the main loop\n",
    "x_train, y_train, rewards = [],[],[]\n",
    "reward_sum = 0\n",
    "episode_nb = 0\n",
    "\n",
    "it=0\n",
    "\n",
    "# main training loop\n",
    "while (True):\n",
    "    cur_input = prepro(observation)\n",
    "    #print(len(cur_input)) — Sanity Check reasons only\n",
    "\n",
    "    x = cur_input - prev_input if prev_input is not None else np.zeros(80 * 80)\n",
    "    prev_input = cur_input\n",
    "\n",
    "    # forward the policy network and sample action according to the probability distribution\n",
    "\n",
    "    proba = model.predict(np.expand_dims(x, axis=1).T)\n",
    "\n",
    "\n",
    "    action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION\n",
    "    y = 1 if action == 2 else 0 # 0 and 1 are our labels# log the input and label to train later\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)# do one step in our environment — This is returned by our environment in OpenAI gym. \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    if done:\n",
    "        it+=1\n",
    "        print(\"done {}-times\".format(it))\n",
    "        history.append(reward_sum)\n",
    "        print(‘At the end of episode’, episode_nb, ‘the total reward was :’, reward_sum)\n",
    "        if episode_nb>=3000 and reward_sum >=-12:\n",
    "        break\n",
    "        else:\n",
    "\n",
    "        # increment episode number\n",
    "        episode_nb += 1\n",
    "\n",
    "        # training\n",
    "        model.fit(x=np.vstack(x_train), y=np.vstack(y_train), verbose=1, sample_weight=discount_rewards(rewards, gamma))\n",
    "\n",
    "\n",
    "        # Reinitialization\n",
    "        x_train, y_train, rewards = [],[],[]\n",
    "        observation = env.reset()\n",
    "        reward_sum = 0\n",
    "        prev_input = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1ed370fe-a81b-41a6-a70f-de33a4e5796a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#buildt model: input layer -> 2x fully connected -> output layer\n",
    "#The input to the neural\n",
    "#network consists is an 84 ×84 ×4 image produced by φ. The first hidden layer convolves 16 8 ×8\n",
    "#filters with stride 4 with the input image and applies a rectifier nonlinearity [10, 18]. The second\n",
    "#hidden layer convolves 32 4 ×4 filters with stride 2, again followed by a rectifier nonlinearity. The\n",
    "#final hidden layer is fully-connected and consists of 256 rectifier units. The output layer is a fully-\n",
    "#connected linear layer with a single output for each valid action. The number of valid actions varied\n",
    "#between 4 and 18 on the games we considered. We refer to convolutional networks trained with our\n",
    "#approach as Deep Q-Networks (DQN). [Playing Atari with Deep Reinforcement Learning] \n",
    "#I will do it with one image bc its quite late and I want to leave something to do for another day:)\n",
    "def build_model(states, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8),input_shape=(84,84,3), strides = 4, activation = 'relu'))\n",
    "    model.add(Conv2D(64, (4, 4), strides = 2, activation = 'relu'))\n",
    "    model.add(Conv2D(64, (3, 3), strides = 1, activation = 'relu'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    #model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "76ca3d20-9bd6-4421-84f8-56930095b913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function SimpleImageViewer.__del__ at 0x0000018F60896AF0>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\erics\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 458, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\Users\\erics\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 454, in close\n",
      "    self.window.close()\n",
      "  File \"C:\\Users\\erics\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\", line 328, in close\n",
      "    super(Win32Window, self).close()\n",
      "  File \"C:\\Users\\erics\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyglet\\window\\__init__.py\", line 857, in close\n",
      "    app.windows.remove(self)\n",
      "  File \"C:\\Users\\erics\\AppData\\Local\\Programs\\Python\\Python39\\lib\\_weakrefset.py\", line 110, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: <weakref at 0x0000018F681B5E50; to 'Win32Window' at 0x0000018F5DDEA6A0>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_11 (Conv2D)          (None, 20, 20, 32)        6176      \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 6)                 3078      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,685,158\n",
      "Trainable params: 1,685,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "05f01ecb-daa7-4bfe-916d-fb7bf5042e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gym wrapper\n",
    "class ProcessFrame84(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(ProcessFrame84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "        \n",
    "def observation(self, obs):\n",
    "    return ProcessFrame84.process(obs)\n",
    "\n",
    "@staticmethod\n",
    "def process(frame):\n",
    "    if frame.size == 210 * 160 * 3:\n",
    "        img = np.reshape(frame, [210, 160,  3]).astype(np.float32)\n",
    "    elif frame.size == 250 * 160 * 3:\n",
    "        img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n",
    "    else:\n",
    "        assert False, \"Unknown resolution.\"      \n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 +  img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33c6f674-a57f-4d1a-8f94-bc754d32bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low,\n",
    "        dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "    \n",
    "    def observation(self, observation):\n",
    "        #self.buffer[:-1] = self.buffer[1:]\n",
    "        #self.buffer[-1] = observation\n",
    "        #return self.buffer\n",
    "        return prepro(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "75179088-255f-4cf2-a9d1-bdf41aad7c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledFloatFrame(gym.ObservationWrapper):\n",
    "     def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "32ae2ec9-0cd2-4c53-85ad-2349edca438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=5000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "81eb31eb-b630-4a4c-9b36-41fcc34dbcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BufferWrapper(gym.make('Pong-v0'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b539314a-0bf6-460c-b221-c6cf23dca14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_11_input to have 4 dimensions, but got array with shape (1, 1, 6400)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9968/2577752998.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mae'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#with tf.device('/gpu:0'): #please use my GPU if it would recognise it :(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\rl\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[1;31m# This is were all of the work happens. We first perceive and compute the action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[1;31m# (forward step) and then use the reward to improve (backward step).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    222\u001b[0m         \u001b[1;31m# Select an action.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_recent_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 224\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mq_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1185\u001b[0m           ' tf.distribute.Strategy.')\n\u001b[0;32m   1186\u001b[0m     \u001b[1;31m# Validate and standardize user data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1187\u001b[1;33m     inputs, _, _ = self._standardize_user_data(\n\u001b[0m\u001b[0;32m   1188\u001b[0m         x, extract_tensors_from_dataset=True)\n\u001b[0;32m   1189\u001b[0m     \u001b[1;31m# If `self._distribution_strategy` is True, then we are in a replica context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2333\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2335\u001b[1;33m     return self._standardize_tensors(\n\u001b[0m\u001b[0;32m   2336\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2337\u001b[0m         \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[1;31m# TODO(fchollet): run static checks with dataset output shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m       x = training_utils_v1.standardize_input_data(\n\u001b[0m\u001b[0;32m   2364\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m           \u001b[0mfeed_input_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\engine\\training_utils_v1.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_shape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m           raise ValueError('Error when checking ' + exception_prefix +\n\u001b[0m\u001b[0;32m    634\u001b[0m                            \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_11_input to have 4 dimensions, but got array with shape (1, 1, 6400)"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
    "#with tf.device('/gpu:0'): #please use my GPU if it would recognise it :(\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d73c9d1-fdd4-459a-93ad-794147a6fbe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
